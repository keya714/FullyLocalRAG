paths:
  pdf_dir: ./corpus
  index_dir: ./index
  gguf_path: ./models/Phi-3-mini-4k-instruct-q4.gguf  # llama.cpp model file

embedding:
  model_name: sentence-transformers/all-MiniLM-L6-v2
  device: cpu

chunking:
  chunk_size: 900
  chunk_overlap: 120

retrieval:
  top_k: 5
  strategy: hybrid        # dense | bm25 | hybrid (works if you used my hybrid retriever)
  bm25_weight: 0.5
  mmr: false
  min_context_chars: 800
  low_recall_threshold: 2

llm:
  model: "llama3.2"   # or any model you `ollama pull`
  n_ctx: 4096
  temperature: 0.3
  max_new_tokens: 512
  stop: []               # e.g., ["<|assistant|>", "</s>"]


guardrails:
  enabled: true
  blocked_topics:
    - self-harm
    - explicit_illegal_howto
    - malware
    - explosives
    - hate
  deny_message: >
    I can’t help with that topic. If you have another question, I’m happy to help with safe, allowed topics.

cli:
  pretty_print: true
  show_sources: true
  max_source_chars: 1000
